{"keys":[{"path":["title"],"id":"title","weight":1,"src":"title","getFn":null},{"path":["body"],"id":"body","weight":1,"src":"body","getFn":null}],"records":[{"i":0,"$":{"0":{"v":"This page has not yet sprouted","n":0.408},"1":{"v":"[Dendron](https://dendron.so/) (the tool used to generate this site) lets authors selective publish content. You will see this page whenever you click on a link to an unpublished page\n\n![](https://foundation-prod-assetspublic53c57cce-8cpvgjldwysl.s3-us-west-2.amazonaws.com/assets/images/not-sprouted.png)","n":0.189}}},{"i":1,"$":{"0":{"v":"Home","n":1},"1":{"v":"## Welcome\n\nThis is the root page for the Software Engineering Team's (SET) notegraph. This content is organized using [Dendron](https://www.dendron.so/).\n\nNew team member onboarding should use the checklist: [[Software Engineering Team Onboarding Checklist|admin.onboading.checklist#software-engineering-team-onboarding-checklist]]","n":0.18}}},{"i":2,"$":{"0":{"v":"Training","n":1}}},{"i":3,"$":{"0":{"v":"Google","n":1}}},{"i":4,"$":{"0":{"v":"Digital Leader","n":0.707},"1":{"v":"\nGoogle Digital Leader Certification training information.\n\n## Details\n\n- Official page: <https://cloud.google.com/certification/cloud-digital-leader>\n- Exam Time Duration: 90 minutes\n- Exam Cost: $99\n- Exam Guide: <https://cloud.google.com/certification/guides/cloud-digital-leader>\n\n## Study Resources\n\n- __Google (Official)__\n  - Cloud Skills Boost: <https://cloud.google.com/training/business#cloud-digital-leader-path>\n  - Coursera: <https://www.coursera.org/professional-certificates/google-cloud-digital-leader-training>\n- __External (Unofficial)__\n  - <https://www.youtube.com/watch?v=UGRDM86MBIQ&ab_channel=freeCodeCamp.org>\n  - <https://youtube.com/watch?v=gddt4n_JEkk&t=4s>\n","n":0.16}}},{"i":5,"$":{"0":{"v":"Resources","n":1}}},{"i":6,"$":{"0":{"v":"Compute","n":1}}},{"i":7,"$":{"0":{"v":"CU Boulder Summit","n":0.577},"1":{"v":"\n## Summary\n\nResearch Computing (RC) through the University of Colorado Boulder (CU Boulder) provides a supercomputer resource called __Summit__. For more information see the following webpage: <https://colorado.edu/rc/resources/summit>.\n\n## CU Anschutz Access\n\nThose with University of Colorado Anschutz (CU Anschutz) may access the Summit supercomputing resource using an [XSEDE account](https://portal.xsede.org/) and configuration help from RC at UCB via the following email: [rc-help@colorado.edu](mailto:rc-help@colorado.edu).\n\nUse the following guide for more information:\n<https://curc.readthedocs.io/en/latest/access/rmacc.html>\n\nGenerally, the access sequence is as follows:\n\n```mermaid\nsequenceDiagram\n    autonumber\n    CU Anschutz Requester ->> portal.xsede.org: Create account\n    portal.xsede.org-->>CU Anschutz Requester: Ask for Account Validation\n    CU Anschutz Requester ->> portal.xsede.org: Validate Account\n    CU Anschutz Requester ->> portal.xsede.org: Configure DUO MFA with XSEDE Acct.\n    CU Anschutz Requester ->> rc-help@colorado.edu: Request RMACC Summit Authorization with XSEDE Username\n    rc-help@colorado.edu -->> CU Anschutz Requester: Authorize RMACC Summit Access\n    CU Anschutz Requester ->> login.xsede.org: local $ ssh -l <username> login.xsede.org\n    CU Anschutz Requester ->> Summit: xsede $ gsissh rmacc-summit\n```\n","n":0.083}}},{"i":8,"$":{"0":{"v":"Tools","n":1}}},{"i":9,"$":{"0":{"v":"Docs","n":1}}},{"i":10,"$":{"0":{"v":"Dendron","n":1}}},{"i":11,"$":{"0":{"v":"Create Note from Jupyter Notebook","n":0.447},"1":{"v":"\n## Summary\n\nThe following note covers how to create Dendron-compatible notes from Jupyter notebook files.\n\n- Jupyter notebooks are JSON files used to compose markdown and code intended to be executed within specialized notebook-compatible environments.\n- Dendron notes are markdown files with frontmatter.\n- Jupytext may be used to create Markdown files from Jupyter notebooks.\n\n## Process\n\n1. Begin with a Jupyter notebook file\n1. Install [Jupytext](https://jupytext.readthedocs.io/en/latest/)\n1. [Pair Jupyter notebook file with markdown file](https://jupytext.readthedocs.io/en/latest/paired-notebooks.html)\n1. Apply [Dendron Doctor: fixFrontMatter](https://wiki.dendron.so/notes/ZeC74FYVECsf9bpyngVMU/#fixfrontmatter) to resulting markdown file.\n1. Enjoy the Jupyter notebook as a Dendron note âœ…\n\nSpecial notes:\n\n- Output from Jupyter notebook cells are not saved by Jupytext when converting to markdown.\n- Each time the notebook is saved, Jupytext pairing will overwrite the Dendron-compatible frontmatter within the markdown file.\n- Despite this, Dendron Doctor: fixFrontMatter appears to be able to retain the note ID.\n","n":0.088}}},{"i":12,"$":{"0":{"v":"Testing","n":1}}},{"i":13,"$":{"0":{"v":"Python","n":1},"1":{"v":"\nCovering some testing tools for Python.\n\n## Testing frameworks or enablers\n\n- [pytest](https://docs.pytest.org/)\n- [unittest](https://docs.python.org/3/library/unittest.html)\n- [nose](https://nose.readthedocs.io/en/latest/index.html)\n- [tox](https://tox.wiki/en/latest/)\n\n## Testing specifics\n\n- [coverage.py](https://github.com/nedbat/coveragepy) Test case code coverage.\n- [mutmut](https://github.com/boxed/mutmut) Mutation testing\n- [hypothesis](https://hypothesis.works/) Property based testing (generating data for tests)\n","n":0.177}}},{"i":14,"$":{"0":{"v":"Linting","n":1}}},{"i":15,"$":{"0":{"v":"Python","n":1},"1":{"v":"\nCovering some linting tools for Python.\n\n## General Code Style (and usually more!)\n\n- [pylint](https://pylint.pycqa.org/en/latest/)\n- [flake8](https://flake8.pycqa.org/en/latest/)\n\n## Security\n\n- [bandit](https://bandit.readthedocs.io/en/latest/)\n- [safety](https://pyup.io/safety/)\n\n## Typing\n\n- [mypy](http://mypy-lang.org/)\n\n## Formatting Specific\n\n- [black](https://black.readthedocs.io/en/stable/)\n- [isort](https://pycqa.github.io/isort/)\n- [vulture](https://github.com/jendrikseipp/vulture)\n- [pycodestyle](https://github.com/PyCQA/pycodestyle)\n\n## Documentation (Python-specific)\n\n- [pyment](https://github.com/dadadel/pyment)\n- [blacken-docs](https://github.com/asottile/blacken-docs)\n","n":0.186}}},{"i":16,"$":{"0":{"v":"Documentation","n":1},"1":{"v":"\nCovering some linting tools for documentation.\n\n## General\n\n- [doc8](https://github.com/PyCQA/doc8): rst linter\n- [markdown-lint](https://github.com/igorshubovych/markdownlint-cli#readme): markdown linting\n","n":0.277}}},{"i":17,"$":{"0":{"v":"Build","n":1}}},{"i":18,"$":{"0":{"v":"Python","n":1}}},{"i":19,"$":{"0":{"v":"Environment","n":1},"1":{"v":"\nBuild tools for Python Environments. In order to ensure reproducibility, a Python developer might make use of specific build tools for their environment. These tools generally try to make sure Python binaries, packages, versions, and cross-dependencies are maintained and source controlled where possible. They also greatly enhance specifications sometimes found within `requirements.txt` files. For more information, see: [Why requirements.txt isn't enough](https://modelpredict.com/wht-requirements-txt-is-not-enough).\n\nBelow is a list of just a few of these tools. Please note: there is not always feature parity between these, it's recommended to only use at your own discretion after investigation.\n\n- [Poetry](https://python-poetry.org/)\n- [PDM](https://pdm.fming.dev/latest/)\n- [Pipenv](https://pipenv.pypa.io/en/latest/)\n- [Conda](https://docs.conda.io/en/latest/)\n- [Pyenv](https://github.com/pyenv/pyenv)\n","n":0.102}}},{"i":20,"$":{"0":{"v":"Dagger","n":1}}},{"i":21,"$":{"0":{"v":"Project Init","n":0.707},"1":{"v":"\n## Summary\n\nProjects which use [Dagger](https://docs.dagger.io/) must be initialized to include dependencies from the core library in order to run. While it's recommended to include these dependencies as part of the project ([reference](https://docs.dagger.io/1225/pushing-plan-dependencies/)), there are sometimes many additional files included as part of this (any possible dependency is included, instead of those referenced only). See the below for getting started with a dagger project.\n\n## Steps\n\n1. Create a `<name>.cue` file to include your custom Dagger work.\n1. Use command `dagger project init` from within the same dir as the cue file.\n1. There are sometimes issues with project init, and you may also need to use `dagger project update` immediately after initializing your project.\n1. Use command `dagger do <action name>` as needed.\n","n":0.092}}},{"i":22,"$":{"0":{"v":"Presentation 2022 07 07","n":0.5},"1":{"v":"\n<!-- slide -->\n\n# Dagger\n\n### <span style=\"color:#aaa; text-align:center;\">Portable Development <br> with Cuelang</span>\n\n<!-- slide -->\n\n## Intro\n\n<br>\n\n[Dagger](https://github.com/dagger/dagger) is a portable development tool implemented through [Cuelang](https://github.com/cue-lang/cue).\n<br>\n\n```mermaid\ngraph LR\n  cue[ *.cue File ] --> dagger[ cmd: dagger do 'action' ]\n  dagger --> buildkit\n  buildkit --> do[perform 'action']\n```\n\n<!-- slide -->\n\n## What's Cuelang?\n\n> <span style=\"text-align:left;float:left;\"> CUE is an open-source data validation language and inference engine with ... many applications, such as data __validation__, data __templating__, __configuration__, __querying__, __code generation__ and even __scripting__.\"</span>\n\n[https://cuelang.org/docs/about/](https://cuelang.org/docs/about/)\n\n<!-- slide -->\n\n## What's Cuelang?\n\nData values and schema together ðŸ˜®\n<table>\n<tr>\n<td>\n\n```json\n// JSON\n// values\n{ \n  \"denver\": {\n    \"name\": \"Denver\",\n    \"state\": \"CO\",\n    \"pop\": 760049,\n    \"capital\": true\n  }\n}\n```\n\n</td>\n<td>\n\n```json\n// CUE\n// values, schema, and more\nbigCity: {\n    // type\n    name: string\n    // default value\n    state: string | *\"CO\"\n    // operators, shorthand\n    pop: >700K\n    // value\n    capital: true\n}\n```\n\n</td>\n</tr>\n</table>\n\n<!-- slide -->\n\n### Cuelang probably deserves <br> it's own conversation! ðŸ™‚\n\n<!-- slide -->\n\n## What's Dagger?\n\n<br>\n\n1. Build a .cue file\n1. Use dagger client to implement (wherever)\n1. Dagger uses [buildkit](https://github.com/moby/buildkit) to perform action\n<br>\n\n```mermaid\ngraph LR\n  cue[ *.cue File ] --> dagger[ cmd: dagger do 'stuff' ]\n  dagger --> buildkit\n  buildkit --> do[perform 'stuff']\n```\n\n<!-- slide -->\n\n## Example Usecase\n\n1. Build a repo .cue file for testing\n1. __Test locally__ with dagger client\n1. __Test remotely__ with [Github Action](https://github.com/dagger/dagger-for-github) (running the same file)\n1. __Celebrate__ ðŸ¥³ <br>(because there weren't surprises with local vs remote testing!)\n\n<!-- slide -->\n\n## Installation: __Dagger Cli__\n\n- macOS: `brew` or `curl + shell`\n- Windows: `psl`, `choco`, or `scoop`\n- Linux: `curl + shell`\n\n<https://docs.dagger.io/install>\n\n<!-- slide -->\n\n## Installation: __Buildkit__\n\nDagger needs a place to invoke buildkit.\n\n- __Docker Desktop__ (built-in compatibility)\n- __Podman__ (some pre-config)\n- __Kubernetes__ (some pre-config)\n- __Remotes__ (BYO remote environment)\n\n<https://docs.dagger.io/1223/custom-buildkit/>\n\n<!-- slide -->\n\n## Demo\n\n[Link here]()\n","n":0.063}}},{"i":23,"$":{"0":{"v":"Buildkit and Podman","n":0.577},"1":{"v":"\n## Summary\n\n[Dagger](https://docs.dagger.io/) by default will attempt to use Docker installations but does not require Docker to run. Some customization without using Docker is covered on the following page: [Customizing your Buildkit installation](https://docs.dagger.io/1223/custom-buildkit/). The following covers how to use Dagger with containerized Buildkit using Podman.\n\n## Steps\n\n1. [Install Podman](https://podman.io/getting-started/installation)\n1. Run Buildkit using the following, modifying as appropriate for your system: `podman run -d --name buildkitd --privileged -p 1234:1234 moby/buildkit:latest --addr tcp://0.0.0.0:1234` ([reference](https://github.com/moby/buildkit#podman) and [issue for podman explicit ports](https://github.com/dagger/dagger/issues/1959#issuecomment-1101547522))\n1. [Install Dagger](https://docs.dagger.io/1200/local-dev)\n1. Set BUILDKIT_HOST environment variable to podman-container://buildkit , for ex: `export BUILDKIT_HOST=tcp://localhost:1234` ([reference](https://docs.dagger.io/1223/custom-buildkit/) and [issue for podman explicit ports](https://github.com/dagger/dagger/issues/1959#issuecomment-1101547522))\n1. Run Dagger commands as necessary.\n","n":0.1}}},{"i":24,"$":{"0":{"v":"Workflow","n":1},"1":{"v":"\nThe usual suspects. Here's a good overview, <https://ploomber.io/blog/survey/>. The ploober document lists the following, but it's also a sales site, so listing tools separately without opinion:\n\n- [Ploomber](https://ploomber.io)\n- [Airflow](https://airflow.apache.org)\n- [Dagster](https://www.dagster.io)\n- DVC (Data Pipelines)\n- Elyra\n- Flyte\n- Kale\n- Kedro\n- Kubeflow pipelines\n- Luigi\n- Metaflow\n- [Prefect](https://www.prefect.io)\n- Tensorflow Extended (TFX)\n\n[DBT](https://www.getdbt.com) is an \"data transformation tool,\" more focused on flowing SQL and database data, but very powerful there.\n\n[Nextflow](https://www.nextflow.io/), a Java tool, is especially liked in the academic research community.\n\nUseful in the academic and research environment, we've run across [SnakeMake](https://snakemake.readthedocs.io/en/stable/index.html).\n\n[Ray Workflows](https://docs.ray.io/en/latest/workflows/basics.html) - Part of the Ray distributed compute project.\n\n[Globus Flows](https://www.globus.org/platform/services/flows) - Part of Globus, a research-focused data management suite.\n","n":0.1}}},{"i":25,"$":{"0":{"v":"K8s","n":1}}},{"i":26,"$":{"0":{"v":"Knative","n":1}}},{"i":27,"$":{"0":{"v":"Install","n":1},"1":{"v":"\n## Prerequisites and Tools\n\nAssumes kubectl is already installed on your workstation.\n\nLens is recommended for watching deployment, service and pod provisioning. https://k8slens.dev/\n\nReview the knative install guide: https://knative.dev/docs/install/operator/knative-with-operators/#install-the-knative-operator\n\nRequires `helm`.\n\nYAML files referenced in this doc are under \\src\\knative-install-resources.\n\nThis doc is written against knative 1.3.1.\n\n## Install Kubernetes Control Plane and Workers\n\nInstall a zone cluster (1 per billing account is free) for MVP.\n\nThis is currently installed as one large cluster, but instead install the cluster, create an autoscaling worker pool, then delete the default pool. \n\nTODO - refactor for the above and include reference to the terraform.\n\n```\ngcloud container --project \"cuhealthai-foundations\" clusters create \"zonal-cluster-1\" --zone \"us-central1-c\" --no-enable-basic-auth --cluster-version \"1.21.6-gke.1500\" --release-channel \"regular\" --machine-type \"e2-medium\" --image-type \"COS_CONTAINERD\" --disk-type \"pd-standard\" --disk-size \"100\" --metadata disable-legacy-endpoints=true --scopes \"https://www.googleapis.com/auth/devstorage.read_only\",\"https://www.googleapis.com/auth/logging.write\",\"https://www.googleapis.com/auth/monitoring\",\"https://www.googleapis.com/auth/servicecontrol\",\"https://www.googleapis.com/auth/service.management.readonly\",\"https://www.googleapis.com/auth/trace.append\" --max-pods-per-node \"110\" --num-nodes \"3\" --logging=SYSTEM,WORKLOAD --monitoring=SYSTEM --enable-ip-alias --network \"projects/cuhealthai-foundations/global/networks/default\" --subnetwork \"projects/cuhealthai-foundations/regions/us-central1/subnetworks/default\" --no-enable-intra-node-visibility --default-max-pods-per-node \"110\" --no-enable-master-authorized-networks --addons HorizontalPodAutoscaling,HttpLoadBalancing,GcePersistentDiskCsiDriver --enable-autoupgrade --enable-autorepair --max-surge-upgrade 1 --max-unavailable-upgrade 0 --enable-shielded-nodes --node-locations \"us-central1-c\"\n```\n\nConfigure .kube/config with credentials for the new cluster.\n\n    gcloud container clusters get-credentials zonal-cluster-1 --zone us-central1-c --project cuhealthai-foundations\n\nCopy the cluster config secret(s) to the password vault.\n\nOnce the cluster is up, fetching nodes should show something like this (example):\n\n```\n$>kubectl get nodes\nNAME                                             STATUS   ROLES    AGE     VERSION\ngke-zonal-cluster-1-default-pool-1915e7e8-2wz5   Ready    <none>   6m43s   v1.21.6-gke.1500\ngke-zonal-cluster-1-default-pool-1915e7e8-dlht   Ready    <none>   6m43s   v1.21.6-gke.1500\ngke-zonal-cluster-1-default-pool-1915e7e8-qcq1   Ready    <none>   6m43s   v1.21.6-gke.1500\n```\n\n## Install knative\n\nInstall the knative operator:\n\n    kubectl apply -f https://github.com/knative/operator/releases/download/knative-v1.3.1/operator.yaml\n\nVerify the installation; the deployment should be available (example):\n\n```\n$>kubectl get deployment knative-operator\nNAME               READY   UP-TO-DATE   AVAILABLE   AGE\nknative-operator   1/1     1            1           69s\n```\n\n### knative-serving\n\nInstall the knative-serving file:\n\n    kubectl apply -f knative-serving.yaml\n\nFetch the external IP address for DNS configuration:\n\n    kubectl --namespace knative-serving get service kourier\n\nIt may take a minute, but eventually the EXTERNAL-IP address will be assigned (example shown).\n\n```\nNAME      TYPE           CLUSTER-IP    EXTERNAL-IP     PORT(S)                      AGE\nkourier   LoadBalancer   10.0.10.151   34.123.218.33   80:31370/TCP,443:31963/TCP   38s\n```\n\nVerify knative-serving is running:\n\n    kubectl get KnativeServing knative-serving -n knative-serving\n\nYou should see READY = True.\n\n```\nNAME              VERSION   READY   REASON\nknative-serving   1.3.0     True\n```\n\nGiven the EXTERNAL-IP from a couple steps above, configure DNS `*.default` and `*.center` to a A records. `default` and `center` map here to k8s namespaces. For every new namespace used with knative, we need to add a separate wildcard A entry.\n\n### knative-eventing\n\nInstall knative-eventing:\n\n    kubectl apply -f knative-eventing.yaml\n\nThere are lots of eventing sources. TBD how we'll use these.\n\n## Configure TLS\n\nFollowing this guide, and assuming the use of the HTTP-01 challenge. https://knative.dev/docs/serving/using-auto-tls/#enabling-auto-tls\n\nAdd the cert-manager controller.\n\n    kubectl apply --filename https://github.com/knative/net-certmanager/releases/download/knative-v1.3.0/release.yaml\n\nInstall cert-manager.\n\n    helm repo add jetstack https://charts.jetstack.io\n    helm repo update\n    helm install cert-manager jetstack/cert-manager --namespace cert-manager --create-namespace --version v1.7.1 --set installCRDs=true\n\nInstall cluster issuer:\n\n    kubectl apply -f letsencrypt-http01-issuer.yaml\n\nCheck to cluster issuer to see that READY = True:\n\n    kubectl get clusterissuer letsencrypt-http01-issuer\n\nAdd a configmap for the certmananger:\n\n    kubectl apply -f config-certmanager.yaml\n\nEnable Auto-TLS. I'm patching below, but with Windows you may have to manually edit.\n\n    kubectl patch configmap config-network --namespace knative-serving -p '{\"data\":{\"auto-tls\":\"Enabled\",\"autocreate-cluster-domain-claims\":\"true\"}}'\n\nInstall a garbage collection policy\n\n    kubectl apply -f gc.yaml\n","n":0.048}}},{"i":28,"$":{"0":{"v":"Diagrams","n":1},"1":{"v":"\nDiagramming can be a useful way to illustrate ideas for yourself or others. Sharing a few tools to this effect below (in no particular order).\n\n- Code-based: generate diagrams using coded dialects\n  - [PlantUML](https://plantuml.com/)\n  - [Mermaid](https://mermaid-js.github.io/mermaid/#/)\n  - [yUML](https://github.com/jaime-olivares/yuml-diagram/wiki)\n- Free-form\n  - [Excalidraw](https://excalidraw.com/)\n  - [Diagrams.net](https://www.diagrams.net/)\n- Aggregate/Mixed:\n  - [Kroki](https://kroki.io/)\n","n":0.149}}},{"i":29,"$":{"0":{"v":"Apps","n":1}}},{"i":30,"$":{"0":{"v":"Mondaydotcom","n":1},"1":{"v":"\n[Monday.com](https://monday.com) is the tool we use for tracking tasks and capturing effort hours. Once a month, tasks are \"posted\" to [[Smartsheet|apps.Smartsheet#^w7ccyzh70hn9]] for later reporting.\n\nAs of this date, their API is not sufficiently rich enough to support using it as the reporting system of record, thus the use of Smartsheet.\n","n":0.143}}},{"i":31,"$":{"0":{"v":"Sketch","n":1}}},{"i":32,"$":{"0":{"v":"Work Visibility Reports","n":0.577},"1":{"v":"\n## Background\n\n```mermaid\nsequenceDiagram\n    Developers->>Management: Raw work visibility\n    Management-->>Developers: Work feedback\n    Management->>Administration: Aggregate work visibility\n    Administration-->>Management: Work feedback\n```\n\n_General sequence of work visibility._\n\nThis document intends to cover solution(s) for work visibility related to the Center for Health AI's (CHAI) projects and internal initiatives. Monday.com is used as a project tracking source, Github is used as the primary source control vehicle. Project work may come in the form of internal continuous improvements, grant-related projects, or other needs CU Anschutz School of Medicine may have.\n\n## Challenge\n\nMonday.com is used as the primary means of work accounting and collaborations but is limited in it's ability to provide administrative visibility. This leads to manual work in creating reports, visualizations, and communications to others where an otherwise automatic solution could be used.\n\n## Porposed Solution(s)\n\n- Each arrow in the diagrams below is roughly equivalent to one \"workflow\" or ETL job which is either triggered or scheduled on a regular basis as needed.\n- \"Datastore\" as seen in the diagrams may be well-suited for object storage with query engine layered above it for utility in downstream deliveries.\n\n### Proposed Flow 1\n\n```mermaid\nflowchart LR\n    monday[Monday.com\\nRaw Data] --> report[Aggregate\\nReport]\n    subgraph Generate\n        report --> storage[Datastore]\n    end\n    subgraph Deliver\n        storage --> PDF\n        PDF --> email\n        storage --> Slack[Slack Channel Post]\n        storage --> Teams[Teams Channel Post]\n    end\n```\n\n_Assumes data is usable or is not needed for archival from Monday.com source._\n\n- Report is generated from raw data and store in CHAI datastore.\n- Report is then sent as Slack, MS Teams posts.\n- Report is also stored in a PDF and sent in an email.\n\n### Proposed Flow 2\n\n```mermaid\nflowchart LR\n    monday[Monday.com\\nRaw Data] --> storage[Datastore]\n    subgraph Generate\n        storage --> report[Aggregate\\nReport]\n        report --> storage\n    end\n    subgraph Deliver\n        storage --> PDF\n        PDF --> email\n        storage --> Slack[Slack Channel Post]\n        storage --> Teams[Teams Channel Post]\n    end\n```\n\n_Assumes data is not usable or is needed for archival from Monday.com source._\n\n- Monday.com raw data is stored in CHAI datastore.\n- Report is generated from raw data and store in CHAI datastore.\n- Report is then sent as Slack, MS Teams posts.\n- Report is also stored in a PDF and sent in an email.\n","n":0.054}}},{"i":33,"$":{"0":{"v":"Smartsheet","n":1},"1":{"v":"\n[Smartsheet](https://www.smartsheet.com/) is a hosted spreadsheet tool we use for administration and operations. ^w7ccyzh70hn9\n\nOur specific use is centered around reporting effort hours and creating monthly pubmed citation reports.\n","n":0.192}}},{"i":34,"$":{"0":{"v":"Admin","n":1}}},{"i":35,"$":{"0":{"v":"Onboading","n":1}}},{"i":36,"$":{"0":{"v":"Checklist","n":1},"1":{"v":"## Software Engineering Team Onboarding Checklist\n\n### Summary\n\nThis page is intended for use during onboarding procedures as a living reference of various items and access required by new or existing employees. ^pstts4z91zvt\n\n### Getting Started\n\n- [ ] Profile Headshot Picture\n- [ ] Professional Bio (3-5 sentences)\n- [ ] Team Introduction\n\n### Required Forms and Payroll\n\n- [ ] COVID Vaccination Status Form\n- [ ] Remote Work Agreement Form\n- [ ] Payroll Direct Deposit\n- [ ] IRS W-4\n- [ ] I9 Verification (Correspondence through email)\n- [ ] [PeopleSoft - My Leave](https://www.cu.edu/employee-services/payroll/self-service/my-leave)\n\n### Benefits\n\n- [ ] Benefits Enrollment / Benefits Orientation\n- [ ] [Tuition Assistance Benefit for Employees](https://www.cu.edu/employee-services/benefits-wellness/current-employee/tuition-assistance/tuition-assistance-benefit)\n- [ ] [CU on Coursera](https://www.cu.edu/employee-services/professional-growth-training/learning/cu-coursera)\n\n### Commuting\n\n- [ ] [RTD EcoPass Information](https://www.cuanschutz.edu/offices/facilities-management/parking-transportation-maps/parking/rtd-eco-pass)\n- [ ] [Car Parking Information](https://www.cuanschutz.edu/offices/facilities-management/parking-transportation-maps/parking/permit-parking)\n- [ ] [Bike Parking Information](https://www.cuanschutz.edu/offices/facilities-management/parking-transportation-maps/parking/bikes-and-scooters)\n\n### Training & Certification\n\n- [ ] Canvas - New Employee Orientation (NEO) Course\n- [ ] Skillsoft - CU: HIPAA Regulations (_scorm12_cu_a00020_0001)\n- [ ] Skillsoft - CU: Administrative Analysis Environment (A2E) Basics (_scorm12_cu_u00202_0001)\n- [ ] [Live â€“ Health Data Compass Orientation](https://www.healthdatacompass.org/data-delivery-services/compass-orientation)\n- [ ] [Coursera - Researcher Management Leadership Training](https://www.coursera.org/learn/researcher-management-leadership-training/) (recommended for those new to academic research)\n\n### General Access\n\n- [ ] Claiming Your Account\n- [ ] Email Address\n- [ ] Phone Communication\n- [ ] Duo Enrollment and Activation\n- [ ] LastPass\n- [ ] Google Workspace Account\n- [ ] Google Project(s) Access\n- [ ] VPN setup\n- [ ] Microsoft Teams Center for Health AI Team Access\n- [ ] Slack\n- [ ] Github Account (New or Linked)\n- [ ] Github Org for Center for Health AI\n- [ ] Github Org Team Access\n- [ ] Monday.com\n- [ ] Zoom Account\n- [ ] University Badging\n- [ ] [Smartsheets Access (Unlicensed)](https://medschool.zendesk.com/hc/en-us/articles/1500003375301-Creating-an-Unlicensed-user-account)\n- [ ] General Setup\n- [ ] [[admin.email-signature]]\n- [ ] Microsoft Profile Image\n- [ ] Slack Profile Image\n- [ ] Zoom Profile Image\n\n### Informational\n\n- [ ] Performance Planning and Reviews\n- [ ] Organization Chart\n- [ ] HR Business Partners\n- [ ] Branding Guidelines\n- [ ] School of Medicine (SOM) Information Services\n- [ ] CU Denver & Anschutz Office of Information Technology (OIT)\n- [ ] SOM Teams for Awareness\n- [ ] Purchasing Process\n- [ ] Grants\n- [ ] Task Sizing and Timing\n- [ ] Principal Investigators (PI's)\n- [ ] Standard Operating Procedures (SOP's)\n- [ ] Data reproducibility\n\n### Research Computing Resources\n\n- [ ] [CU Boulder Research Computing](https://colorado.edu/rc/resources)\n- [ ] [RMACC](https://rmacc.org/)\n- [ ] Supercomputers\n- [ ] PetaLibrary\n- [ ] Globus\n\n### Team Policies and Cadence\n\n- [ ] Vacation Time\n- [ ] Sick Time\n- [ ] Communication\n- [ ] Seminar, Chalk Talk, Lunch+Learn Attendance\n- [ ] Generalized Source Control Cadence\n- [ ] Project Management Reporting\n- [ ] Weekly Team Meetings\n\n### Data Governance\n\n- [ ] HIPAA Data\n- [ ] FERPA Data\n- [ ] Data Source Approvals\n- [ ] Common CU Data Sources\n","n":0.048}}},{"i":37,"$":{"0":{"v":"Email Signature","n":0.707},"1":{"v":"\nEmail signatures are standardized within the department. Use the following Word document reference point (logging in with your CU credentials), copy the content (including images with links) and modify it to match your personal information where appropriate.\n\n[Email Signature Block Instructions](https://olucdenver.sharepoint.com/:w:/s/CenterforHealthAI939-SoftwareEngineering/EXsfKEe9-HBKtLROR-PWAp4BEc0Ij_nQUDegPIG8ej94sQ?e=zV0PYh)\n","n":0.158}}}]}
